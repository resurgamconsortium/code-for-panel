{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development of fault detection procedure in welding using Machine Learning (ML). The fault detection procedure is based on Remaining Useful Life (RUL) prediction of welding tool and damage identification of welding process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modules to be imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data into the editor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 of the project - Remaining Useful Lifetime (RUL) prediction of the welding tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data into the IDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Train_data='loc of train data'\n",
    "Test_data_DF='Loc of test data'\n",
    "Train_data_DF= pd.read_csv(Train_data,encoding='cp1252', parse_dates=True)\n",
    "Train_data_DF.head()\n",
    "Test_data_DF= pd.read_csv(Test_data_DF,encoding='cp1252', parse_dates=True)\n",
    "Test_data_DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the shape of the data and check for any null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Train_data_DF.shape)\n",
    "print(list(Train_data_DF.columns))\n",
    "print(Train_data_DF.isna().sum())\n",
    "print(Test_data_DF.shape)\n",
    "print(list(Test_data_DF.columns))\n",
    "print(Test_data_DF.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the statistics to check for faulty sensos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Train_data_DF.describe().T)\n",
    "print(Test_data_DF.describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlier removal steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(Train_data_DF.columns):\n",
    "    q75,q25 = np.percentile(Train_data_DF[i],[75,25])\n",
    "    intr_qr = q75-q25\n",
    "    max = q75+(1.5*intr_qr)\n",
    "    min = q25-(1.5*intr_qr)\n",
    "    Train_data_DF.loc[Train_data_DF[i] < min, i] = np.nan\n",
    "    Train_data_DF.loc[Train_data_DF[i] > max, i] = np.nan\n",
    "    Train_data_DF = Train_data_DF.dropna(axis = 0)\n",
    "print(Train_data_DF.shape)\n",
    "for i in list(Test_data_DF.columns):\n",
    "    q75,q25 = np.percentile(Test_data_DF[i],[75,25])\n",
    "    intr_qr = q75-q25\n",
    "    max = q75+(1.5*intr_qr)\n",
    "    min = q25-(1.5*intr_qr)\n",
    "    Test_data_DF.loc[Test_data_DF[i] < min, i] = np.nan\n",
    "    Test_data_DF.loc[Test_data_DF[i] > max, i] = np.nan\n",
    "    Test_data_DF = Test_data_DF.dropna(axis = 0)\n",
    "print(Test_data_DF.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualisations of each features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,20))\n",
    "l=1\n",
    "for i in list(Train_data_DF.columns):\n",
    "    plt.subplot(len(list(Train_data_DF.columns)), 1, l)\n",
    "    plt.plot(Train_data_DF[i])\n",
    "    plt.title(i, y=0.5, loc='right')\n",
    "    l += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling the data / Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "Ttrain = Train_data_DF.copy()\n",
    "Ttest = Test_data_DF.copy()\n",
    "Ttrain.iloc[:,0:-1] = scaler.fit_transform(Ttrain.iloc[:,0:-1])\n",
    "Ttest.iloc[:,0:-1] = scaler.fit_transform(Ttest.iloc[:,0:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop the metadata, label and features separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainLabel=Ttrain['Cycle']\n",
    "Ttrain=Ttrain.drop(['Elektrode Stant','Elektrode number', 'lower bound','upper bound','Cycle'], axis=1)\n",
    "TestLabel=Ttest['Cycle']\n",
    "Ttest=Ttest.drop(['Elektrode Stant','Elektrode number', 'lower bound','upper bound', 'Cycle'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling using XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=1, colsample_bytree=1)\n",
    "# fit model\n",
    "model.fit(Ttrain, TrainLabel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K fold cross validation to inspect the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, Ttrain, Ttest, scoring='mean_squared_error', cv=cv, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict the test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(Ttest)\n",
    "rmse = np.sqrt(MSE(TestLabel, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 of the project damage detection in the welding process "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=pd.read_csv('Loc of the data')\n",
    "features=a[['RMS','COUN','ENER','DURATION']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heat Map is inspected to ensure diversity of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_label=['RMS', 'COUN', 'ENER', 'DURATION']\n",
    "Y_label=['RMS', 'COUN', 'ENER', 'DURATION']\n",
    "sns.heatmap(X.corr(),xticklabels=X_label,yticklabels=Y_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scale the data and use k-NN algorithm to calculate the distance_desc. The plot of distance_desc provides an elbo shaped graph from which a hyperparameter is derived "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(StandardScaler().fit_transform(features))\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "neighbors = 8\n",
    "# X_embedded is your data\n",
    "nbrs = NearestNeighbors(n_neighbors=neighbors ).fit(X)\n",
    "distances, indices = nbrs.kneighbors(X)\n",
    "#distance_desc = sorted(distances[:,-1], reverse=True)\n",
    "mean_dis=[]\n",
    "for i in range(len(distances)):\n",
    "    mean_dis.append(distances[i].mean())\n",
    "distance_desc = sorted(mean_dis, reverse=True)\n",
    "\n",
    "plt.plot(distance_desc)\n",
    "plt.ylabel('Distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the hyperparameter eps=0.53 (from K-NN algorithm), min_samples=8 and data is feed to DBSCAN algorithm which detects the outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=0.53, min_samples=8).fit(X)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA is used to reduce the dimension of the data to 2D for the visualisation of the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "data_pca = pca.transform(X)\n",
    "data_pca = pd.DataFrame(data_pca,columns=['PC1','PC2','PC3','PC4'])\n",
    "data_pca.head()\n",
    "import matplotlib.pyplot as plt\n",
    "pd.DataFrame(pca.explained_variance_ratio_).plot.bar()\n",
    "plt.legend('')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Explained Varience')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot of the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_pca['PC1'], data_pca['PC2'], c=core_samples_mask, cmap=plt.cm.bwr, s=20)\n",
    "plt.legend('Cluster')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9add3d81e0cff8bd701294482085be7795e0f9e1faebc4c3e916df25ed0f3bb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
